From 7b4e76077ab74d1660782213940bb193a8d98155 Mon Sep 17 00:00:00 2001
From: Peter Halverson <peter.halverson@gmail.com>
Date: Thu, 11 Jan 2018 16:47:54 -0500
Subject: [PATCH] initial checkin

---
 README.md                           |  11 +-
 lambda/marchon.py                   |  17 ++-
 monitoring/SETUP.md                 |  15 +++
 monitoring/format_request_events.py |  50 +++++++++
 monitoring/monitor_lambda_runs.py   | 202 ++++++++++++++++++++++++++++++++++++
 monitoring/requirements.txt         |   0
 monitoring/template.yaml            |  20 ++++
 monitoring/test_monitoring.py       | 119 +++++++++++++++++++++
 8 files changed, 422 insertions(+), 12 deletions(-)
 create mode 100644 monitoring/SETUP.md
 create mode 100644 monitoring/format_request_events.py
 create mode 100644 monitoring/monitor_lambda_runs.py
 create mode 100644 monitoring/requirements.txt
 create mode 100644 monitoring/template.yaml
 create mode 100644 monitoring/test_monitoring.py

diff --git a/README.md b/README.md
index ebbf667..844c423 100644
--- a/README.md
+++ b/README.md
@@ -17,26 +17,31 @@ Mapbox GL JS doesn't allow access to the entire feature set, only features in th
 
 The image resizing code uses [Pillow](https://github.com/python-pillow/Pillow), which contains platform-specific C code. When deploying, make sure you include the Linux version in the zip file. The easiest way to do this is to create the deployment package on Linux; [get a Docker container](https://medium.freecodecamp.org/escaping-lambda-function-hell-using-docker-40b187ec1e48) if you don't have access to a Linux box. Alternatively, you can `pip install Pillow -t linux-pillow` on Linux, then copy the resulting packages into the zip. You can do this just once, then freshen `marchon.py` as needed.
 
-create directory for Linux deployment package
+create directories for Linux deployment packages
 
     mkdir lambda-linux
     mkdir lambda-linux/lambda
     cp lambda/marchon.py lambda/requirements.txt lambda-linux/lambda
+    mkdir lambda-linux/monitoring
+    cp monitoring/*.py lambda-linux/monitoring
 
 run Ubuntu docker container with python 3.6
 
     docker run -v /path-to/marchon-map/lambda-linux:/lambda-linux -it --rm tomersha/docker-ubuntu-14.04-python-3.6.2
 
-activate python 3.6, install zip, install packages, and create zip
+activate python 3.6, install zip, install packages, and create zip archives
 
     pyenv shell 3.6.2
     apt-get install zip
     cd lambda-linux/lambda
     pip install -r requirements.txt -t .
     zip ../linux-lambda.zip -r .
+    cd ../monitoring
+    pip install -r requirements.txt -t .
+    zip ../monitoring.zip -r .
     exit
 
-upload `linux-lambda.zip` to AWS
+upload `linux-lambda.zip` and `monitoring.zip` to AWS
 
 ## GeoJSON to map
 
diff --git a/lambda/marchon.py b/lambda/marchon.py
index e888990..331b1dc 100644
--- a/lambda/marchon.py
+++ b/lambda/marchon.py
@@ -14,10 +14,9 @@ from PIL import Image
 
 from action_network import get_events_from_events_campaign, make_key
 
-logging.basicConfig(level=logging.DEBUG, format='%(levelname)s %(message)s')
+logging.basicConfig(level=logging.DEBUG)
 log = logging.getLogger(__name__)
 
-
 def read_sheet(sheet_range, fields, location_idx, affiliate):
     log.info('\nload sheet %s with %s', os.environ['SHEET_ID'],
              os.environ['GOOGLE_API_KEY'])
@@ -57,7 +56,7 @@ def read_sheet(sheet_range, fields, location_idx, affiliate):
             log.debug('row %s\t%s\t%s',
                       len(rows) + 1, props['name'], props['location'])
         else:
-            log.warning('WARNING\tskipping %s: no location', (props['name']))
+            log.warning('Skipping %s: no location', (props['name']))
         idx += 1
     log.info('read %s rows from sheet', len(rows))
     return rows
@@ -150,13 +149,13 @@ def get_geodata(sheet, keys, countries=None):
             feature = response['features'][0]
             log.info('geocode %s\n\t%s', key, feature)
             if feature['relevance'] < 0.75:
-                log.warning('WARNING\terror geocoding %s', key)
+                log.warning('Error geocoding %s', key)
                 continue
             sheet[key]['geometry'] = response['features'][0]['geometry']
         else:
             if key in sheet:
                 del sheet[key]
-            log.warning('WARNING\terror geocoding %s', key)
+            log.warning('Error geocoding %s', key)
 
 
 def merge_data(sheet, dataset):
@@ -219,12 +218,12 @@ def resize_photo(service, file):
     resized.save(img_bytes, format=file_ext.upper())
     img_bytes.seek(0)
     s3 = boto3.resource('s3')
-    log.info(
-        s3.Object('ragtag-marchon', filename).put(
+    response = s3.Object('ragtag-marchon', filename).put(
             Body=img_bytes.read(),
             ContentType=file['mimeType'],
             ACL='public-read',
-            Expires=(datetime.now() + timedelta(hours=24 * 7))))
+            Expires=(datetime.now() + timedelta(hours=24 * 7)))
+    log.info(response)
     return filename
 
 
@@ -264,7 +263,7 @@ def update_photos(dataset):
             log.info('%s saved to %s', photo['name'], url)
             dataset[key]['properties']['photoUrl'] = url
         except:
-            log.error('ERROR resizing photo')
+            log.error('Error resizing photo')
             traceback.print_exc()
 
 
diff --git a/monitoring/SETUP.md b/monitoring/SETUP.md
new file mode 100644
index 0000000..f4bcd54
--- /dev/null
+++ b/monitoring/SETUP.md
@@ -0,0 +1,15 @@
+# Monitoring setup
+
+1. Ensure target service has cloudwatch log update permissions. verify log output!
+2. Update or create lambda execution role, requires
+    * lambda execution
+    * cloudwatch log access
+    * SNS publish  
+3. Update or create SNS topic
+4. Add subscription(s) to topic (with filters if desired, attributes TBD)
+5. Create monitoring lambda function
+    * zipped deployment package
+    * Environment variables
+    * execution role
+6. Ensure monitoring lambda has read permissions on service logs and publish perms on SNS topic
+7. Create cloudwatch log metric on target service log group, bound to monitoring lambda 
\ No newline at end of file
diff --git a/monitoring/format_request_events.py b/monitoring/format_request_events.py
new file mode 100644
index 0000000..ee29fbd
--- /dev/null
+++ b/monitoring/format_request_events.py
@@ -0,0 +1,50 @@
+""""
+Functions for formatting AWS Lambda run logs
+"""
+import datetime
+import re
+
+def create_message_subject(info):
+    """
+    Create the message subject
+    """
+    base = '%s request completed' % info['name']
+    if info['errors'] > 0:
+        return '%s with ERRORS!' % base
+    elif info['warnings'] > 0:
+        return '%s with WARNINGS!' % base
+    else:
+        return base
+        
+def format_log_event(event):
+    """
+    Format an AWS lambda log event.
+    """
+    message = event['message']
+    ts = datetime.datetime.fromtimestamp(event['timestamp'] / 1000).strftime('%H:%M:%S')
+    if message.startswith('START'):
+        return '%s %-7s\n' % (ts, 'START')
+    elif message.startswith('END'):
+        return '%s %-7s\n' % (ts, 'END')
+    elif message.startswith('REPORT'):
+        return '' # ignore report events
+    else:
+        match = re.match('\[([A-Z]+)\]\t[^\t]+\t[^\t]+\t([^\n]+)\n', message)
+        if match:
+            return '%s %-7s %s\n' % (ts,match.group(1),match.group(2))
+        else:
+            return '%s %s\n' % (ts, message)
+    
+def create_message_body(info):
+    """
+    Create the message body, including summary information and formatted event log.
+    """
+    return 'Execution results for %s (%s)\n\n' % (info['name'], info['requestId']) + \
+      'Execution time: %s seconds\n' % datetime.timedelta(milliseconds=info['duration']).total_seconds() + \
+      '%d errors\n' % info['errors'] + \
+      '%d warnings\n' % info['warnings'] + \
+      '\nExecution Log\n\n' + \
+      ''.join(map(format_log_event, info['events']))
+      
+if __name__ == "main":
+    raise NotImplementedError('This module cannot be executed.')
\ No newline at end of file
diff --git a/monitoring/monitor_lambda_runs.py b/monitoring/monitor_lambda_runs.py
new file mode 100644
index 0000000..0de84f6
--- /dev/null
+++ b/monitoring/monitor_lambda_runs.py
@@ -0,0 +1,202 @@
+"""
+Collect log events for completed AWS lambda runs and publish status report
+"""
+from __future__ import print_function
+
+import base64
+import json
+import logging
+import os
+import zlib
+
+import boto3
+
+# message formatting in separate module, perhaps one day we can support customizing
+# format through some kind of context-specific formatting hook.
+from format_request_events import create_message_subject, create_message_body
+
+# flag for test/debug, disables SNS publish
+dry_run = False
+
+def get_env_var(name, default_value = None):
+    """Get the value of an environment variable, if defined"""
+    if name in os.environ:
+        return os.environ[name]
+    elif default_value is not None:
+        return default_value
+    else:
+        raise RuntimeError('Required environment variable %s not found' % name)
+
+# Get configuration from environment variables 
+log_level = get_env_var('LOG_LEVEL', 'INFO')
+service_name = get_env_var('SERVICE_NAME')
+log_group_name = get_env_var('SERVICE_LOG_GROUP')
+topic_url = get_env_var('TARGET_TOPIC_URL')
+
+# Configure local logging
+logging.basicConfig(level=log_level)
+log = logging.getLogger()
+
+def decompress_string(input):
+    """
+    Convert base64-encoded, compressed data to a string
+    """
+    data = base64.b64decode(input)
+    return zlib.decompress(data,47,4096).decode()
+
+def unpack_subscription_event(input):
+    """
+    Convert and parse cloudwatch log subscription event
+    """
+    payload = decompress_string(input['awslogs']['data'])
+    event = json.loads(payload)
+    return event
+
+def get_run_events(requestId):
+    """
+    Get cloudwatch log events for the specified lambda request
+    
+    Assumes log events are formatted with the default Lambda log format, i.e. '<level> <timestamp> <requestid> ...".
+    """
+    filter = '[level,ts,id=%s,...]' % requestId
+    logs = boto3.client('logs')
+    results = logs.filter_log_events(
+        logGroupName=log_group_name, 
+        filterPattern=filter,
+        interleaved=True)
+    return results['events']
+
+def analyze_run_events(events):
+    """
+    Collect information about request execution from log events.
+    """
+    errors = 0
+    warnings = 0
+    startts = 0
+    endts = 0
+    for event in events:
+        if 'message' in event:
+            message = event['message']
+            if message.startswith('START'):
+                startts = event['timestamp']
+            elif message.startswith('END'):
+                endts = event['timestamp']
+            elif message.startswith('[ERROR]'):
+                errors = errors + 1
+            elif message.startswith('[WARNING]'):
+                warnings = warnings + 1
+    duration = endts - startts
+    return { 'duration': duration, 'errors': errors, 'warnings': warnings }
+
+def create_topic_message(info):
+    """
+    Create a report message for a request execution 
+    """
+    subject = create_message_subject(info)
+    defaultMessage = create_message_body(info)
+    # TODO define alternate messages for other protocols, e.g. SMS
+    return (subject, defaultMessage)
+
+def publish_run_info(info):
+    """
+    Publish job execution report to SNS topic.
+    
+    If dry_run is True, dumps subject and message to stdout instead of
+    publishing to the topic.
+    """
+    (subject,message) = create_topic_message(info)
+    if info['errors'] > 0:
+        status = 'error'
+    elif info['warnings'] > 0:
+        status = 'warning'
+    else:
+        status = 'success'
+    attributes = {
+         'name': {
+             'DataType': 'String',
+             'StringValue': info['name']
+         },
+         'status': {
+             'DataType': 'String',
+             'StringValue': status
+         },
+         'errors': {
+             'DataType': 'String',
+             'StringValue': str(info['errors'])
+         },
+         'warnings': {
+             'DataType': 'String',
+             'StringValue': str(info['warnings'])
+         },
+    }
+    global dry_run
+    if dry_run:
+        # dump message to stdout
+        print(subject)
+        print(message)
+        print(json.dumps(attributes))
+        response = { 'MessageId': '12345' }
+    else: 
+        # publish to topic
+        sns = boto3.client('sns')
+        response = sns.publish(
+                     TopicArn=topic_url,
+                     Subject=subject,
+                     Message=message,
+                     MessageAttributes=attributes)
+    log.info('Published message %s to target topic %', response['MessageId'], topic_url)
+    return response
+
+def process_lambda_run(requestId):
+    """
+    Process CloudWatch log events for a lambda function run
+    """
+    log.debug('Processing log events for lambda request %s', requestId)
+    events = get_run_events(requestId)
+    info = analyze_run_events(events)
+    publish_run_info(dict(info, 
+                          requestId=requestId,
+                          name=service_name, 
+                          events=events))
+
+def get_request_ids(events):
+    """
+    Get request IDs from a set of lambda log events
+    """
+    ids = []
+    for event in events:
+     if ('extractedFields' in event):
+         fields = event['extractedFields']
+         if 'type' in fields and fields['type'] == 'END' and 'requestId' in fields:
+             ids.append(fields['requestId'])
+    # shouldn't be any dupes, but we check anyway
+    assert len(ids) == len(set(ids)), "Found duplicate request ids"
+    return ids
+   
+def process_lambda_events(events):
+    """
+    Process a set of Lambda log events, running `process_lambda_run` for each END event.
+    
+    It's possible that the log subscription could be configured to send all events 
+    for a particular run to the handler, but I haven't seen anything that guarantees this. So
+    for now we only look at END requests, then explicitly collect all the others through a 
+    filter_log_events query. It's highly recommended to add a filter to the log subscription 
+    that only looks at 'END' events, to avoid including other request events that will only be 
+    discarded here.
+    """
+    for id in get_request_ids(events):
+        process_lambda_run(id)
+
+#
+# lambda entry point
+#
+def lambda_handler(input, context):
+    """
+    Process a CloudWatch Log trigger.
+    """
+    global dry_run
+    dry_run = os.getenv('DRY_RUN', 'false').lower() == 'true'
+    subscription_event = unpack_subscription_event(input)
+    log.debug('Event data: %s', json.dumps(subscription_event))
+    process_lambda_events(subscription_event['logEvents'])
+    return 'Mischief managed.'
\ No newline at end of file
diff --git a/monitoring/requirements.txt b/monitoring/requirements.txt
new file mode 100644
index 0000000..e69de29
diff --git a/monitoring/template.yaml b/monitoring/template.yaml
new file mode 100644
index 0000000..82a53cd
--- /dev/null
+++ b/monitoring/template.yaml
@@ -0,0 +1,20 @@
+﻿AWSTemplateFormatVersion: '2010-09-09'
+Transform: 'AWS::Serverless-2016-10-31'
+Description: Monitor log output from lambda execution requests
+Resources:
+  monitorlambdalogs:
+    Type: 'AWS::Serverless::Function'
+    Properties:
+      Handler: monitor_lambda_runs.lambda_handler
+      Runtime: python3.6
+      CodeUri: .
+      Description: Monitor log output from lambda execution requests
+      MemorySize: 128
+      Timeout: 3
+      Role: 'arn:aws:iam::187976421381:role/BaseLambdaRole'
+      Environment:
+        Variables:
+          SERVICE_NAME: Hello-World
+          TARGET_TOPIC_URL: 'arn:aws:sns:us-east-1:187976421381:slack-awe-test'
+          SERVICE_LOG_GROUP: /aws/lambda/hello-world
+          LOG_LEVEL: INFO
diff --git a/monitoring/test_monitoring.py b/monitoring/test_monitoring.py
new file mode 100644
index 0000000..6f123d3
--- /dev/null
+++ b/monitoring/test_monitoring.py
@@ -0,0 +1,119 @@
+"""
+Unit tests for execution monitoring code
+"""
+
+import unittest
+import json
+import gzip
+import base64
+import io
+
+import monitor_lambda_runs as uut
+
+# test data
+
+test_subscription_event = { 
+   'logStream': '2018/01/04/[$LATEST]610fa3c1703d4ac4975ce6375d918c69', 
+   'messageType': 'DATA_MESSAGE', 
+   'logEvents': [ 
+       { 'extractedFields':  { 'dummy': 'RequestId:', 'type': 'END', 'requestId': '1844c0df-f186-11e7-92ed-29e116819200' }, 
+         'timestamp': 1515097568250, 
+         'message': 'END RequestId: 1844c0df-f186-11e7-92ed-29e116819200\n', 
+         'id': '33787804820456610596729582913350286578116803276340658177' }, 
+       { 'extractedFields': { 'dummy': 'RequestId:', 'type': 'END', 'requestId': 'f65dbf9d-f186-11e7-a442-2f62625222a6' }, 
+         'timestamp': 1515097568871, 
+         'message': 'END RequestId: f65dbf9d-f186-11e7-a442-2f62625222a6\n', 
+        'id': '33787804834305373365017099884243967625431435771554496517' },
+       { 'extractedFields': { 'dummy': 'RequestId:', 'type': 'END', 'requestId': '7fd0c2ca-f18d-11e7-98dc-6d74fadb1fba' }, 
+         'timestamp': 1515097569546, 
+         'message': 'END RequestId: 7fd0c2ca-f18d-11e7-98dc-6d74fadb1fba\n', 
+         'id': '33787804849358376374025270504780577459469079788091277321' } 
+    ], 
+   'owner': '187976421381', 
+   'subscriptionFilters': [ 'LambdaStream_monitor-lambda-logs' ], 
+   'logGroup': '/aws/lambda/hello-world' 
+}
+
+test_lambda_events = [
+    { 
+      'timestamp': 1515097568871, 
+      'message': 'START RequestId: f65dbf9d-f186-11e7-a442-2f62625222a6\n', 
+    },
+    { 
+      'timestamp': 1515097569871, 
+      'message': '[WARNING] 1515097569871 f65dbf9d-f186-11e7-a442-2f62625222a6\n', 
+    },
+    { 
+      'timestamp': 1515097570871,
+      'message': '[WARNING] 1515097570871 f65dbf9d-f186-11e7-a442-2f62625222a6\n', 
+    },
+    { 
+      'timestamp': 1515097571871, 
+      'message': '[INFO] 1515097571871 f65dbf9d-f186-11e7-a442-2f62625222a6\n', 
+    },
+    { 
+      'timestamp': 1515097572871, 
+      'message': '[WARNING] 1515097572871 f65dbf9d-f186-11e7-a442-2f62625222a6\n', 
+    },
+    { 
+      'timestamp': 1515097573871, 
+      'message': '[ERROR] 1515097573871 f65dbf9d-f186-11e7-a442-2f62625222a6\n', 
+    },    
+    { 
+      'timestamp': 1515097574871, 
+      'message': 'END RequestId: f65dbf9d-f186-11e7-a442-2f62625222a6\n', 
+    },
+
+]
+
+def compress_string(str):
+    return base64.b64encode(gzip.compress(str.encode()))
+
+def pack_subscription_event(event):
+    return { 'awslogs': { 'data': compress_string(json.dumps(event)) } }
+
+class MonitoringTests(unittest.TestCase):
+
+    def test_decompress_data(self):
+        original = 'ABCDEFGHIJKLMNOP'
+        encoded = compress_string(original) # 'H4sIAFmkV1oAA3N0cnZxdXP38PTy9vH18w8AAE3/6OAQAAAA'
+        result = uut.decompress_string(encoded)
+        self.assertEqual(result, original)   
+    
+    def test_unpack_subscription_event(self):
+        packed = pack_subscription_event(test_subscription_event)
+        result = uut.unpack_subscription_event(packed)
+        self.assertEqual(result, test_subscription_event)
+    
+    def test_get_request_ids(self):
+        events = test_subscription_event['logEvents']
+        result = uut.get_request_ids(events)
+        self.assertSetEqual(set(result), {
+            '1844c0df-f186-11e7-92ed-29e116819200',
+            '7fd0c2ca-f18d-11e7-98dc-6d74fadb1fba',
+            'f65dbf9d-f186-11e7-a442-2f62625222a6'
+          })
+        
+    def test_analyze_run_events(self):
+        result = uut.analyze_run_events(test_lambda_events)
+        self.assertDictEqual(result, {
+            'duration': 6000,
+            'errors': 1,
+            'warnings': 3                        
+            })
+        
+    def test_create_topic_message(self):
+        info = {
+            'name': 'Hello World',
+            'requestId': 'f65dbf9d-f186-11e7-a442-2f62625222a6',
+            'duration': 6000,
+            'errors': 1,
+            'warnings': 3,
+            'events': test_lambda_events
+            }
+        (subject,message) = uut.create_topic_message(info)
+        self.assertRegex(subject, '''^Hello World request completed with ERRORS.+''')
+        self.assertRegex(message, '''Execution results for Hello World''')                    
+    
+if __name__ == '__main__':
+    unittest.main()
\ No newline at end of file
-- 
2.10.1

